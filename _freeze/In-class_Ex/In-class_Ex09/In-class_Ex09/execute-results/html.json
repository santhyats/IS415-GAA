{
  "hash": "ae5acafa1bdabf0566ff300229a53cad",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"In-class Exercise 9\"\nauthor: \"Santhya Selvan\"\ndate: \"23 September 2024\"\ndate-modified: \"last-modified\"\nexecute: \n  eval: true\n  echo: true\n  freeze: true\n---\n\n\n\n## 9.0 Getting started\n\n#### 9.0.1 Loading the packages\n\nFirstly, we will download the necessary packages into our R environment.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(spdep, sp, tmap, sf, ClustGeo, cluster, factoextra, NbClust, tidyverse, GGally)\n```\n:::\n\n\n\n#### 9.0.2 Downloading the data\n\nWe will now load in the data to our R environment. Since they are in the form of rds files, we will use the *read_rds()* function of the **readr** package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshan_sf<- read_rds(\"data/rds/shan_sf.rds\")\nshan_ict <- read_rds(\"data/rds/shan_ict.rds\")\nshan_sf_clusters <- read_rds(\"data/rds/shan_sf_cluster.rds\")\n```\n:::\n\n\n\n## 9.1 Hierarchical Clustering\n\nExplanation for the code chunks below:\n\nLine 1) The first step to compute hierarchical clusters is to create a proximity matrix. *dist()* function from base R is an example of a function that can be used to derive this proximity matrix. Proximity Matrix calculates the numerical distance between all the variables.\n\nLine 2) This line uses the *hclust()* function of base R functions, that helps us to derive the clusters from the proximity matrix.\n\nLine 3) *cutree()* is a function that directly consumes an object of the h-class (hierarchical cluster object). This line helps us to derive the groups we will be using for the dendrogram. This function then cuts the hierarchical clustering tree so as to get k specified number of clusters. The *as.factor()* function converts the segments of the cut tree into categorical values. value of k=6 is derived from the optimal number of clusters that we found using the gap statistic method.\n\nFor project, it is advisable to let the value of k be an user input, rather than hardcoding the value. A slider can be used on the user interface to enable them to toggle between different number of clusters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nproxmat<- dist(shan_ict, method = 'euclidean')\nhclust_ward<-hclust(proxmat, method='ward.D')\ngroups<- as.factor(cutree(hclust_ward, k = 6))\n```\n:::\n\n\n\n#### 9.1.1 Visualising the Hierarchical Clusters\n\n*select(-c())* will enable us to drop the columns selected.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshan_sf_clusters<- cbind(shan_sf, \n                as.matrix(groups)) %>% \n  rename(`CLUSTER` = `as.matrix.groups.`) %>% \n  select(-c(3:4, 7:9)) %>% \n  rename(TS = TS.x)\n```\n:::\n\n\n\nNow, we will plot the dendrogram to visualise our hierarchical clustering.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hclust_ward, cex=0.5)\nrect.hclust(hclust_ward, k=6, border = 2:5)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nWe will also plot out the clusters on our map and colour code them. We will use the *qtm()* function to achieve this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqtm(shan_sf_clusters, \"CLUSTER\")\n```\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nHowever, this output only considers the similarity in their attributes and does not consider the spatial relationship between the spatial units, and this shown by the clusters being spatially divided over the space. Hence, it will be better if we can come up with clusters that considers both the attributes and the spatial relationships between the spatial units. This is where the SKATER approach comes in.\n\n## 9.2 Spatially Constrained Clustering using the SKATER Approach\n\nWe will first compute the minimum spanning tree.\n\n#### 9.2.3 Computing minimum spanning tree\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshan.nb <- poly2nb(shan_sf)\nlcosts <- nbcosts(shan.nb, shan_ict)\nshan.w <- nb2listw(shan.nb, lcosts, style='B')\nsummary(shan.w)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshan.mst <- mstree(shan.w)\n```\n:::\n\n\n\nWe are now ready to visualise our minimum spanning tree.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npts <- st_coordinates(st_centroid(shan_sf))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: st_centroid assumes attributes are constant over geometries\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(st_geometry(shan_sf),\n     border = gray(.5))\n\nplot.mst(shan.mst, pts, col=\"blue\",\n         cex.lab = 0.7, \n         cex.circles = 0.005,\n         add = TRUE)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nNext, we will derive our clusters using the SKATER method by passing in our minimum spanning tree.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskater.clust6 <- skater(edges = shan.mst[, 1:2],\n                        data = shan_ict,\n                        method = \"euclidean\",\n                        ncuts = 5)\n```\n:::\n\n\n\nWe can now map out our clusters with their neighbourhood relationships on the map once again.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(st_geometry(shan_sf),\n     border=gray(.5))\n\nplot(skater.clust6, \n     pts, \n     cex.lab=.7, \n     groups.colors = c(\"red\", \"green\", \"brown\", \"blue\", \"pink\"),\n     cex.circles = 0.005,\n     add = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nWe will further visualise our clusters on a choropleth map.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngroups_mat = as.matrix(skater.clust6$groups)\nshan_sf_spatialcluster <- cbind(shan_sf_clusters, as.factor(groups_mat)) %>% \n  rename(`skater_CLUSTER` = `as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"skater_CLUSTER\")\n```\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nSKATER approach to clustering is considered a hard clustering method as all classifications is based on the minimum spanning tree.\n\nClustGeo, on the other hand, is considered a soft clustering method as it allows us to choose the proportion of the attribute and spatial constraints to be used.\n\n## 9.3 Spatially Constrained Clustering Method using ClustGeo\n\n9.3.1 Calculating the distance matrix\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist <- st_distance(shan_sf, shan_sf)\ndistmat <- as.dist(dist)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncr <- choicealpha(proxmat, distmat, \n                  range.alpha = seq(0, 1, 0.1),\n                  K=6, graph = TRUE)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\n\nWe will need save the ClustGeo output\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclust6 <- hclustgeo(proxmat, distmat, alpha=0.2)\ngroups <- as.factor(cutree(clust6, k=6))\nshan_sf_ClustGeo <- cbind(shan_sf, \n                          as.matrix(groups)) %>% \n  rename(`clustGeo` = `as.matrix.groups.`)\n```\n:::\n\n\n\nFinally, we can map our clusters on the map\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqtm(shan_sf_ClustGeo, \"clustGeo\")\n```\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n#### 9.3.1 Characterising the clusters\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggparcoord(data = shan_sf_ClustGeo,\n           columns = c(17:20),\n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE,\n           title = \"Multiple Parallel Coordinates Plots od ICT Variables by Cluster\") +\n  facet_grid(~ clustGeo) + \n  theme(axis.text.x = element_text(angle = 30))\n```\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "In-class_Ex09_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}